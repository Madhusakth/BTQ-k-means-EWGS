dataset: cifar10	arch: resnet20_quant	num_workers: 4	seed: None	batch_size: 32	epochs: 400	optimizer_m: Adam	optimizer_q: Adam	lr_m: 0.001	lr_q: 1e-05	lr_m_end: 0.0	lr_q_end: 0.0	decay_schedule_m: 150-300	decay_schedule_q: 150-300	momentum: 0.9	weight_decay: 0.0001	lr_scheduler_m: cosine	lr_scheduler_q: cosine	gamma: 0.1	QWeightFlag: True	QActFlag: True	weight_levels: 256	act_levels: 256	baseline: False	bkwd_scaling_factorW: 0.0	bkwd_scaling_factorA: 0.0	use_hessian: True	update_every: 10	gpu_id: 1	log_dir: test_2_2_8_8_part_32bs/	load_pretrain: True	pretrain_path: ../results/ResNet20_CIFAR10/fp/checkpoint/best_checkpoint.pth	btq: True	training_flag: False	eval: False	weighted: False	bits: 5	cv_block_size: 6	pw_fc_block_size: 4	sensitivity: True	
Files already downloaded and verified
The number of parameters :  269940
Pretrained full precision weights are initialized
# total params: 269940
# model params: 269850
# quantizer params: 90
ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=64, out_features=10, bias=True)
)
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 000 	 Test accuracy: 10.01 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 001 	 Test accuracy: 13.44 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 002 	 Test accuracy: 14.38 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 003 	 Test accuracy: 15.73 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 004 	 Test accuracy: 19.759999999999998 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 005 	 Test accuracy: 19.509999999999998 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 006 	 Test accuracy: 19.939999999999998 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 007 	 Test accuracy: 14.790000000000001 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 008 	 Test accuracy: 32.84 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 009 	 Test accuracy: 23.21 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:10<00:21, 10.65s/it] 67%|██████▋   | 2/3 [00:26<00:13, 13.66s/it]100%|██████████| 3/3 [00:44<00:00, 15.48s/it]100%|██████████| 3/3 [00:44<00:00, 14.69s/it]


scaleA
 [0.012565852206718665, 0.007988228162605927, 0.006763413627804017, 0.009436322723621604, 0.003883740911181673, 0.005331781461426096, 0.00509403160109496, 0.00884285458192964, 0.0102333251820025, 0.007979636607549407, 0.006923483150568351, 0.013071692326844693, 0.005822722472222333, 0.0126738655763066, 0.0011225918900543556, 0.0011969263048192587, 0.0051951036742125005, 0.0035406241251730824]
scaleW
 [0.0951452890715447, 0.09426502404128634, 0.13090185234822307, 0.07833120507263672, 0.07570126854503528, 0.039813554493432414, 0.054472994559004735, 0.03876014952090229, 0.05073932602038464, 0.021788199871856043, 0.05437395415525018, 0.04036219229989662, 0.025967360382751686, 0.01648440099122091, 0.0010480165324191692, 0.0, 0.009082971123242195, 0.003951731416009375]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 010 	 Test accuracy: 28.67 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 011 	 Test accuracy: 28.83 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 012 	 Test accuracy: 28.48 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 013 	 Test accuracy: 22.91 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 014 	 Test accuracy: 38.21 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 015 	 Test accuracy: 33.12 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 016 	 Test accuracy: 33.31 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 017 	 Test accuracy: 23.25 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 018 	 Test accuracy: 44.31 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 019 	 Test accuracy: 43.81 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:13<00:26, 13.36s/it] 67%|██████▋   | 2/3 [00:28<00:14, 14.50s/it]