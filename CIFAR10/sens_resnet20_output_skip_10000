dataset: cifar10	arch: resnet20_quant	num_workers: 4	seed: None	batch_size: 256	epochs: 400	optimizer_m: Adam	optimizer_q: Adam	lr_m: 0.004	lr_q: 4e-05	lr_m_end: 0.0	lr_q_end: 0.0	decay_schedule_m: 150-300	decay_schedule_q: 150-300	momentum: 0.9	weight_decay: 0.0001	lr_scheduler_m: cosine	lr_scheduler_q: cosine	gamma: 0.1	QWeightFlag: True	QActFlag: True	weight_levels: 256	act_levels: 256	baseline: False	bkwd_scaling_factorW: 0.0	bkwd_scaling_factorA: 0.0	use_hessian: True	update_every: 10	gpu_id: 1	log_dir: test_4_partition_skip_10000/	load_pretrain: True	pretrain_path: ../results/ResNet20_CIFAR10/fp/checkpoint/best_checkpoint.pth	btq: True	training_flag: False	eval: False	weighted: False	bits: 5	cv_block_size: 6	pw_fc_block_size: 4	sensitivity: True	
Files already downloaded and verified
The number of parameters :  269940
Pretrained full precision weights are initialized
# total params: 269940
# model params: 269850
# quantizer params: 90
ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=64, out_features=10, bias=True)
)
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 000 	 Test accuracy: 10.299999999999999 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 001 	 Test accuracy: 15.440000000000001 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 002 	 Test accuracy: 18.64 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 003 	 Test accuracy: 22.34 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 004 	 Test accuracy: 28.439999999999998 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 005 	 Test accuracy: 38.59 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 006 	 Test accuracy: 26.029999999999998 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 007 	 Test accuracy: 34.57 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 008 	 Test accuracy: 34.510000000000005 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 009 	 Test accuracy: 35.24 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:26<02:52, 86.28s/it] 67%|   | 2/3 [02:29<01:12, 72.86s/it]100%|| 3/3 [03:30<00:00, 67.55s/it]100%|| 3/3 [03:31<00:00, 70.34s/it]


scaleA
 [0.013999894649676888, 0.00800233645624638, 0.006739607396938192, 0.008496580753586294, 0.0034530964811944204, 0.005634371822685298, 0.0056674384465917665, 0.011322471057716825, 0.007770453514139, 0.009287050981918754, 0.007221310545487279, 0.010081839343936614, 0.007284772550540666, 0.01619932656859946, 0.00932292759189209, 0.009507001718797634, 0.008474615359279607, 0.007481274950296751]
scaleW
 [0.33093628641468675, 0.2888431635985278, 0.35941260104181433, 0.19136339275739714, 0.18746841856687532, 0.13321366897535966, 0.1745184666929718, 0.1301616158571419, 0.12073360871703336, 0.07862716439460295, 0.12444087571335359, 0.0772137726394974, 0.07895720008451516, 0.07329443871011955, 0.05641130430858996, 0.044924814362237574, 0.04939363135536425, 0.02402392546640804]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 010 	 Test accuracy: 33.339999999999996 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 011 	 Test accuracy: 51.51 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 012 	 Test accuracy: 47.48 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 013 	 Test accuracy: 30.43 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 014 	 Test accuracy: 46.94 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 015 	 Test accuracy: 49.65 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 016 	 Test accuracy: 32.04 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 017 	 Test accuracy: 51.61 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 018 	 Test accuracy: 36.71 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 019 	 Test accuracy: 38.629999999999995 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:16<02:32, 76.01s/it] 67%|   | 2/3 [02:11<01:03, 63.88s/it]100%|| 3/3 [03:18<00:00, 65.37s/it]100%|| 3/3 [03:18<00:00, 66.19s/it]


scaleA
 [0.01327637965528163, 0.006598206459355847, 0.006046490312318557, 0.008082137045596516, 0.003664818067145203, 0.006011619955884814, 0.00504957094721466, 0.009133764595666116, 0.006549310772564036, 0.00875127622602994, 0.005541812255960239, 0.007957895423871791, 0.006312728422279767, 0.014791721297949484, 0.009226289094704526, 0.011143863497565508, 0.00853061506803904, 0.008599467629568423]
scaleW
 [0.2552001515433731, 0.24599808089897593, 0.2752399081911215, 0.1735732722144583, 0.20147642061138038, 0.12007081586234952, 0.15035257475913477, 0.10253257531219269, 0.09305272330099608, 0.0638406067554123, 0.08958073291767436, 0.04777710626768116, 0.05812893235968066, 0.0519054276598081, 0.04938207341978305, 0.039251669631263725, 0.04106952487133972, 0.02216924474744052]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 020 	 Test accuracy: 41.11 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 021 	 Test accuracy: 35.21 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 022 	 Test accuracy: 46.69 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 023 	 Test accuracy: 23.29 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 024 	 Test accuracy: 42.230000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 025 	 Test accuracy: 37.29 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 026 	 Test accuracy: 53.32 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 027 	 Test accuracy: 35.730000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 028 	 Test accuracy: 51.910000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 029 	 Test accuracy: 52.01 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:09<02:18, 69.36s/it] 67%|   | 2/3 [02:23<01:12, 72.02s/it]100%|| 3/3 [03:39<00:00, 74.06s/it]100%|| 3/3 [03:39<00:00, 73.26s/it]


scaleA
 [0.013522062444642787, 0.0070239286236495535, 0.006457427722688786, 0.009434493582955925, 0.004114146509084901, 0.006408859577659177, 0.005184000110329247, 0.009737355264273457, 0.008329887168816595, 0.010372368305207874, 0.006885001884615302, 0.00997645690212949, 0.007073677134030519, 0.015701168046380082, 0.009230308102286923, 0.011305609377537848, 0.009597961212200365, 0.009459317922496982]
scaleW
 [0.22031500843614318, 0.20228540099796744, 0.25009154297944997, 0.17821024372950448, 0.21010960565878492, 0.11426613713105477, 0.1277204956523303, 0.09184460823711549, 0.10285735005418994, 0.060332417667883574, 0.09855414387318835, 0.05344536595340029, 0.05824968638310508, 0.050032072464307166, 0.04480011585379892, 0.0321844934821495, 0.040111848883139144, 0.02078393172455367]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 030 	 Test accuracy: 50.28 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 031 	 Test accuracy: 29.32 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 032 	 Test accuracy: 33.660000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 033 	 Test accuracy: 51.41 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 034 	 Test accuracy: 52.56999999999999 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 035 	 Test accuracy: 42.96 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 036 	 Test accuracy: 60.6 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 037 	 Test accuracy: 59.41 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 038 	 Test accuracy: 24.81 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 039 	 Test accuracy: 32.47 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:56<01:52, 56.21s/it] 67%|   | 2/3 [02:02<01:02, 62.10s/it]100%|| 3/3 [03:13<00:00, 66.23s/it]100%|| 3/3 [03:13<00:00, 64.53s/it]


scaleA
 [0.01313219474886506, 0.006957561489930447, 0.006185808606427228, 0.008884723645199976, 0.00455256651823134, 0.006363472204087385, 0.005040851040242968, 0.00940571231203118, 0.0073211614546723645, 0.00914722742175707, 0.006355898003626394, 0.010570265427378316, 0.0066187903975299155, 0.015258701743281915, 0.009627483178096549, 0.01093727385588466, 0.008248033560576818, 0.008716413353783255]
scaleW
 [0.20548464432939584, 0.20283387276270456, 0.23690552366652232, 0.15558251219456318, 0.19113306582763165, 0.10038855117800426, 0.11917684675116262, 0.0908849782157326, 0.08850658537432647, 0.060980936284197505, 0.08973499085894714, 0.059631067093364354, 0.05840644995541078, 0.049868705253823864, 0.04491027932972733, 0.031332938124745156, 0.03247439751333029, 0.018916360807145635]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 040 	 Test accuracy: 54.83 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 041 	 Test accuracy: 46.53 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 042 	 Test accuracy: 66.25 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 043 	 Test accuracy: 50.43 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 044 	 Test accuracy: 59.45 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 045 	 Test accuracy: 42.63 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 046 	 Test accuracy: 59.08 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 047 	 Test accuracy: 51.0 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 048 	 Test accuracy: 32.43 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 049 	 Test accuracy: 48.99 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:43<01:27, 43.79s/it] 67%|   | 2/3 [01:31<00:46, 46.13s/it]100%|| 3/3 [02:50<00:00, 60.91s/it]100%|| 3/3 [02:50<00:00, 56.70s/it]


scaleA
 [0.014197634530851705, 0.0073304147420591844, 0.0065368531105155945, 0.00984864611772703, 0.004956256450718813, 0.007368682464512603, 0.006041192264429146, 0.011313715636865049, 0.008089835773490935, 0.010534376974387388, 0.006994826457727722, 0.012568833218860752, 0.007072019964672878, 0.016392613797007065, 0.009273377382302642, 0.011620946152467873, 0.008714538747708086, 0.009867532924679354]
scaleW
 [0.22484260753602878, 0.24985172791096058, 0.2544766904390889, 0.18472415386606852, 0.21714179181250573, 0.13090242177754838, 0.14428464892342008, 0.1076858170434507, 0.09627355343219877, 0.06482592482835271, 0.10331266796103367, 0.06495612151261353, 0.06555812749755359, 0.053922860557501444, 0.047734665679841415, 0.03634842677617001, 0.04012929061773766, 0.01876635057084401]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 050 	 Test accuracy: 45.43 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 051 	 Test accuracy: 64.88000000000001 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 052 	 Test accuracy: 54.43 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 053 	 Test accuracy: 54.85 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 054 	 Test accuracy: 37.88 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 055 	 Test accuracy: 51.18000000000001 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 056 	 Test accuracy: 60.96 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 057 	 Test accuracy: 46.800000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 058 	 Test accuracy: 45.910000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 059 	 Test accuracy: 53.89000000000001 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:14<02:29, 74.88s/it] 67%|   | 2/3 [02:17<01:07, 67.91s/it]100%|| 3/3 [03:16<00:00, 63.75s/it]100%|| 3/3 [03:16<00:00, 65.58s/it]


scaleA
 [0.014490580828663023, 0.0070796851739154025, 0.006468973208493693, 0.00996617021981719, 0.004615530447981868, 0.00659189856531852, 0.004688948810866549, 0.008948128958906022, 0.006293036189510188, 0.008767768664246743, 0.005985521122063515, 0.010500737110128576, 0.006433884478741241, 0.015525983576930788, 0.007386831362048446, 0.011724162780928934, 0.006287685908824048, 0.008790271598522756]
scaleW
 [0.24904841417994017, 0.20979711233253706, 0.25418637745324496, 0.18421205919018044, 0.19083433148553472, 0.09707458068278808, 0.12157874706183235, 0.08698489025862734, 0.07935666599676894, 0.05347174925518409, 0.09481462489619812, 0.0583464199688452, 0.058165787165399795, 0.05052710705820077, 0.04049315183184478, 0.03569335224091449, 0.02459870921272488, 0.01507739870926976]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 060 	 Test accuracy: 55.22 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 061 	 Test accuracy: 49.36 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 062 	 Test accuracy: 59.84 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 063 	 Test accuracy: 48.02 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 064 	 Test accuracy: 56.25 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 065 	 Test accuracy: 46.5 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 066 	 Test accuracy: 51.6 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 067 	 Test accuracy: 61.519999999999996 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 191
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 068 	 Test accuracy: 51.349999999999994 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 069 	 Test accuracy: 49.309999999999995 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:07<02:15, 67.93s/it] 67%|   | 2/3 [02:29<01:15, 75.73s/it]100%|| 3/3 [03:28<00:00, 68.15s/it]100%|| 3/3 [03:28<00:00, 69.43s/it]


scaleA
 [0.012206619241462048, 0.006356847751215505, 0.005978146239919359, 0.008927768172449441, 0.0034578112016196705, 0.0057041924743059845, 0.005149080690333385, 0.009048058528997904, 0.006877706246411118, 0.009364790309182785, 0.005926023982881517, 0.009441882341657706, 0.006395810936887135, 0.014707990944233688, 0.0074781346766201835, 0.010162623751523657, 0.008555990095707918, 0.01140255346981025]
scaleW
 [0.21447673692060423, 0.19606837134313057, 0.23593906453511157, 0.1612291130589405, 0.19849356086763018, 0.09463051344657576, 0.1332726640313875, 0.09890865120608623, 0.09241787180768625, 0.056947196555297896, 0.09407869954259716, 0.050892132626028645, 0.06263817039413584, 0.04593481548791301, 0.0398714342216999, 0.026642083952363685, 0.03855317605915733, 0.02192183649716965]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 070 	 Test accuracy: 58.17 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 071 	 Test accuracy: 46.949999999999996 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 072 	 Test accuracy: 52.790000000000006 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 073 	 Test accuracy: 30.45 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 074 	 Test accuracy: 68.5 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 075 	 Test accuracy: 35.699999999999996 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 076 	 Test accuracy: 49.919999999999995 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 077 	 Test accuracy: 56.36 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 078 	 Test accuracy: 57.410000000000004 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 079 	 Test accuracy: 67.25 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:59<01:59, 59.72s/it] 67%|   | 2/3 [02:03<01:01, 61.95s/it]100%|| 3/3 [03:05<00:00, 62.07s/it]100%|| 3/3 [03:05<00:00, 61.83s/it]


scaleA
 [0.012126451820894993, 0.005880142539737474, 0.006186699158767757, 0.008507016696686487, 0.0047148990844366615, 0.0062102071757125865, 0.00460709495888201, 0.009264621727995666, 0.006973391052737875, 0.009485356968209267, 0.005734565463878916, 0.009870336907164752, 0.005912096264738765, 0.015005955558832667, 0.006125144350454197, 0.010247720338812516, 0.008004213794239897, 0.00989358348144373]
scaleW
 [0.2079439230355676, 0.17893145815171763, 0.22457842196675382, 0.14076817545489254, 0.20589009655622503, 0.10121718469240044, 0.11077227698656304, 0.09354901987646025, 0.08182895570913595, 0.05909548751593244, 0.0861181798959737, 0.05163355223694436, 0.054816833451825996, 0.04975887889640209, 0.031574194917706445, 0.030546853499558237, 0.03560141471029374, 0.017581004117049385]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 080 	 Test accuracy: 53.86 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 081 	 Test accuracy: 59.209999999999994 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 082 	 Test accuracy: 49.220000000000006 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 083 	 Test accuracy: 65.23 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 084 	 Test accuracy: 57.19 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 085 	 Test accuracy: 44.49 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 086 	 Test accuracy: 59.17 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 087 	 Test accuracy: 61.69 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 088 	 Test accuracy: 49.26 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 089 	 Test accuracy: 50.49 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:08<02:16, 68.40s/it] 67%|   | 2/3 [02:09<01:04, 64.31s/it]100%|| 3/3 [03:22<00:00, 68.18s/it]100%|| 3/3 [03:22<00:00, 67.55s/it]


scaleA
 [0.01228280963621764, 0.006221882890045623, 0.006267535193963091, 0.009025713245194364, 0.004738326224116743, 0.006039778268984965, 0.005606519527197467, 0.010468832946010009, 0.006520565697659524, 0.008531289144126356, 0.0068307511860106465, 0.00999980704531439, 0.005649593758577748, 0.013904118180258662, 0.007524341967104641, 0.012124131237892159, 0.00799265007086005, 0.011345948928285629]
scaleW
 [0.22379528025210726, 0.2082270147486698, 0.24560978983125312, 0.16411790665365164, 0.20406989755421642, 0.10177691330048588, 0.15061053851832548, 0.10598181131209079, 0.08235205482855426, 0.05267481875881025, 0.09775588903767414, 0.053038380694049224, 0.0519258170897246, 0.04327974784773886, 0.036996209028663185, 0.03321888185881436, 0.032234723539490175, 0.019037568099832747]

conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 090 	 Test accuracy: 63.65 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 091 	 Test accuracy: 50.870000000000005 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 092 	 Test accuracy: 62.93 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 093 	 Test accuracy: 60.79 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 094 	 Test accuracy: 56.26 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 095 	 Test accuracy: 65.35 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 096 	 Test accuracy: 42.83 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 097 	 Test accuracy: 52.15 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 098 	 Test accuracy: 66.57 %
conv1.weight torch.Size([16, 3, 3, 3])
Quantizing weight: conv1.weight torch.Size([16, 3, 3, 3])
number of params conv1.weight 432
Skipping weights: conv1.weight
k-means on skipped weights (1, 432)
final weight shape: torch.Size([16, 3, 3, 3])
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv1.weight 2304
product quantization with block_size:  6 layer1.0.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.0.conv2.weight 2304
product quantization with block_size:  6 layer1.0.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv1.weight 2304
product quantization with block_size:  6 layer1.1.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.1.conv2.weight 2304
product quantization with block_size:  6 layer1.1.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv1.weight 2304
product quantization with block_size:  6 layer1.2.conv1.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
number of params layer1.2.conv2.weight 2304
product quantization with block_size:  6 layer1.2.conv2.weight
k-means on skipped weights (6, 384)
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
number of params layer2.0.conv1.weight 4608
product quantization with block_size:  6 layer2.0.conv1.weight
k-means on skipped weights (6, 768)
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.0.conv2.weight 9216
product quantization with block_size:  6 layer2.0.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv1.weight 9216
product quantization with block_size:  6 layer2.1.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.1.conv2.weight 9216
product quantization with block_size:  6 layer2.1.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv1.weight 9216
product quantization with block_size:  6 layer2.2.conv1.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
number of params layer2.2.conv2.weight 9216
product quantization with block_size:  6 layer2.2.conv2.weight
k-means on skipped weights (6, 1536)
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
number of params layer3.0.conv1.weight 18432
product quantization with block_size:  6 layer3.0.conv1.weight
Partition k-means
torch.Size([6, 3072]) (3072, 6) 120
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.0.conv2.weight 36864
product quantization with block_size:  6 layer3.0.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv1.weight 36864
product quantization with block_size:  6 layer3.1.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.1.conv2.weight 36864
product quantization with block_size:  6 layer3.1.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv1.weight 36864
product quantization with block_size:  6 layer3.2.conv1.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
number of params layer3.2.conv2.weight 36864
product quantization with block_size:  6 layer3.2.conv2.weight
Partition k-means
torch.Size([6, 6144]) (6144, 6) 120
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 099 	 Test accuracy: 54.49 %
  0%|          | 0/3 [00:00<?, ?it/s]