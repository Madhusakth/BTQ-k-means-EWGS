dataset: cifar10	arch: resnet20_quant	num_workers: 4	seed: None	batch_size: 256	epochs: 400	optimizer_m: Adam	optimizer_q: Adam	lr_m: 0.004	lr_q: 4e-05	lr_m_end: 0.0	lr_q_end: 0.0	decay_schedule_m: 150-300	decay_schedule_q: 150-300	momentum: 0.9	weight_decay: 0.0001	lr_scheduler_m: cosine	lr_scheduler_q: cosine	gamma: 0.1	QWeightFlag: True	QActFlag: True	weight_levels: 256	act_levels: 256	baseline: False	bkwd_scaling_factorW: 0.0	bkwd_scaling_factorA: 0.0	use_hessian: True	update_every: 10	gpu_id: 1	log_dir: test_orig/	load_pretrain: True	pretrain_path: ../results/ResNet20_CIFAR10/fp/checkpoint/best_checkpoint.pth	btq: True	training_flag: False	eval: False	weighted: False	cv_block_size: 6	pw_fc_block_size: 4	sensitivity: False	
Files already downloaded and verified
The number of parameters :  269940
Pretrained full precision weights are initialized
# total params: 269940
# model params: 269850
# quantizer params: 90
ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): QBasicBlock(
      (conv1): QConv(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): LambdaLayer()
    )
    (1): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (2): QBasicBlock(
      (conv1): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): QConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=64, out_features=10, bias=True)
)
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 000 	 Test accuracy: 17.84 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 001 	 Test accuracy: 20.59 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 002 	 Test accuracy: 22.55 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 003 	 Test accuracy: 25.259999999999998 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 004 	 Test accuracy: 25.430000000000003 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 005 	 Test accuracy: 25.15 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 006 	 Test accuracy: 37.78 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 007 	 Test accuracy: 33.800000000000004 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 008 	 Test accuracy: 39.08 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 009 	 Test accuracy: 36.4 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:02<02:05, 62.83s/it] 67%|   | 2/3 [02:08<01:04, 64.68s/it]100%|| 3/3 [03:14<00:00, 65.10s/it]100%|| 3/3 [03:14<00:00, 64.81s/it]


scaleA
 [0.01296132532612812, 0.007312514134928107, 0.006620820583119111, 0.009097228806750762, 0.0038795247558970694, 0.0053229368913889985, 0.004548630516608323, 0.009302182502613629, 0.006365232644874592, 0.008250148763442364, 0.008091766266010629, 0.01114670699205733, 0.008059058159089577, 0.016638814742894782, 0.011442773713686037, 0.014142674913603095, 0.009840520977543938, 0.010933441966890112]
scaleW
 [0.28829079884494396, 0.24903229990406497, 0.2935328836218421, 0.20876225884508928, 0.20524686631446754, 0.11001312379978523, 0.12364530946315914, 0.10288889374136899, 0.0935734206749343, 0.06780031606903847, 0.1268130867498576, 0.0800078954818928, 0.06871588970604764, 0.05555387614848601, 0.05529901250325233, 0.047978707485438456, 0.04522860513571955, 0.026172028128058775]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 010 	 Test accuracy: 30.070000000000004 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 011 	 Test accuracy: 53.36 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 012 	 Test accuracy: 56.720000000000006 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 013 	 Test accuracy: 52.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 014 	 Test accuracy: 47.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 015 	 Test accuracy: 50.39 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 016 	 Test accuracy: 39.56 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 017 	 Test accuracy: 57.97 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 018 	 Test accuracy: 66.45 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 019 	 Test accuracy: 43.76 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:07<02:14, 67.48s/it] 67%|   | 2/3 [02:11<01:05, 65.22s/it]100%|| 3/3 [03:13<00:00, 63.80s/it]100%|| 3/3 [03:13<00:00, 64.42s/it]


scaleA
 [0.014300476921384024, 0.007250365126920765, 0.006605616602831308, 0.008494572805298568, 0.005190240988132895, 0.005987729970268187, 0.004598022437727453, 0.008885394395958642, 0.006143126662856817, 0.007964839255101684, 0.005887832478982506, 0.008244898367656356, 0.007810559082604224, 0.016980758246389723, 0.010630753882151213, 0.012823264649292773, 0.008699137976962534, 0.009049295186205767]
scaleW
 [0.2684115611723867, 0.22993834528133547, 0.27785170372083623, 0.17556714854340225, 0.2101002314084313, 0.11559972275265391, 0.11426527507824043, 0.09398673800833712, 0.0854295819457803, 0.06111468895198174, 0.09280651071630756, 0.04924016643074671, 0.06084706434590203, 0.048221251093260094, 0.04372004016153316, 0.03584031310228121, 0.03472814798627603, 0.01566673363423605]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 020 	 Test accuracy: 43.38 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 021 	 Test accuracy: 40.6 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 022 	 Test accuracy: 62.029999999999994 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 023 	 Test accuracy: 43.04 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 024 	 Test accuracy: 56.82000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 025 	 Test accuracy: 51.23 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 026 	 Test accuracy: 54.459999999999994 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 027 	 Test accuracy: 69.63000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 028 	 Test accuracy: 49.21 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 029 	 Test accuracy: 63.190000000000005 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:12<02:24, 72.06s/it] 67%|   | 2/3 [02:18<01:08, 68.91s/it]100%|| 3/3 [03:20<00:00, 65.59s/it]100%|| 3/3 [03:20<00:00, 66.81s/it]


scaleA
 [0.014018908504780122, 0.006964720411207259, 0.006796416775567718, 0.009376742120885085, 0.005120286745640925, 0.005706008452059177, 0.004731412294663794, 0.008393388212232332, 0.005894543588086064, 0.008171306041035097, 0.005885958819621302, 0.009445620101971634, 0.0073560169319906695, 0.015929449675512984, 0.010555930289472574, 0.014746196167701321, 0.009212552749219371, 0.010780059748053829]
scaleW
 [0.22289582511845574, 0.2201770725882435, 0.25354827049450607, 0.17502753295779275, 0.204221114739409, 0.09090856449772906, 0.12460405112225442, 0.0857453879276322, 0.07489902714650092, 0.04878509751102372, 0.08795590384691349, 0.059036618992077204, 0.055448360202023074, 0.0466007295393085, 0.040953554144306674, 0.03611306038382058, 0.0363636584643717, 0.02065645016308416]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 030 	 Test accuracy: 45.839999999999996 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 031 	 Test accuracy: 71.71 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 032 	 Test accuracy: 67.15 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 033 	 Test accuracy: 58.209999999999994 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 034 	 Test accuracy: 60.91 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 035 	 Test accuracy: 37.580000000000005 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 036 	 Test accuracy: 61.51 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 037 	 Test accuracy: 68.94 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 038 	 Test accuracy: 67.89 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 039 	 Test accuracy: 56.69 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:11<02:23, 71.93s/it] 67%|   | 2/3 [02:00<00:58, 58.48s/it]100%|| 3/3 [03:32<00:00, 73.60s/it]100%|| 3/3 [03:32<00:00, 70.88s/it]


scaleA
 [0.011480974486103945, 0.006335679667135035, 0.005434185110415564, 0.008419618083542014, 0.0037540587802604767, 0.005577887119080045, 0.004951466944217273, 0.008704291366403069, 0.005059536884900863, 0.007714536254304718, 0.005347921648526745, 0.008167158890050139, 0.007719007250631109, 0.01781720544764447, 0.009601889980277547, 0.011354147747290303, 0.008570395508790933, 0.010766555155442897]
scaleW
 [0.23638828324715547, 0.2081701214192272, 0.22346826039043446, 0.16968633239685396, 0.17102111406184048, 0.09278187812839021, 0.12963607991251982, 0.09361368588552904, 0.06777797582856399, 0.045441948124392346, 0.08368522008632019, 0.03857694831257524, 0.05818922810972952, 0.04916582348030465, 0.04017891660028656, 0.0285356683814975, 0.03228846262825167, 0.017297232358616178]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 040 	 Test accuracy: 65.14999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 041 	 Test accuracy: 70.47 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 042 	 Test accuracy: 71.32 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 043 	 Test accuracy: 50.43 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 044 	 Test accuracy: 57.53 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 045 	 Test accuracy: 57.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 046 	 Test accuracy: 59.98 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 047 	 Test accuracy: 59.28 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 048 	 Test accuracy: 61.970000000000006 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 049 	 Test accuracy: 75.53999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:17<02:35, 77.98s/it] 67%|   | 2/3 [02:38<01:19, 79.58s/it]100%|| 3/3 [04:12<00:00, 86.14s/it]100%|| 3/3 [04:12<00:00, 84.22s/it]


scaleA
 [0.010911112246615383, 0.00575638476983405, 0.005692415551021499, 0.008322103589031125, 0.004142298557545498, 0.005130374608691668, 0.00426047026721506, 0.00786744985073036, 0.004743392415412613, 0.007375245547942336, 0.006007396948300142, 0.008509746797290508, 0.005942647125578237, 0.013431041794390993, 0.00732626247224943, 0.011277415905660585, 0.0066716028652067634, 0.00919155054948486]
scaleW
 [0.16567480523397926, 0.1660152590041839, 0.2193734323956754, 0.1355625129397767, 0.1683632532712626, 0.082696134325277, 0.10452067678323172, 0.0734386653836661, 0.05995817204366166, 0.04824412060199052, 0.08480506984787899, 0.048712641636934305, 0.04270831220348085, 0.03579180410482986, 0.029497586953641925, 0.02628377990110808, 0.025369889294480193, 0.014587098840259263]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 050 	 Test accuracy: 67.07 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 051 	 Test accuracy: 66.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 052 	 Test accuracy: 72.44 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 053 	 Test accuracy: 59.95 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 054 	 Test accuracy: 54.339999999999996 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 055 	 Test accuracy: 66.96 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 056 	 Test accuracy: 59.18 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 057 	 Test accuracy: 58.660000000000004 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 058 	 Test accuracy: 54.790000000000006 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 059 	 Test accuracy: 61.4 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:06<02:13, 66.70s/it] 67%|   | 2/3 [02:28<01:15, 75.40s/it]100%|| 3/3 [03:33<00:00, 70.67s/it]100%|| 3/3 [03:33<00:00, 71.09s/it]


scaleA
 [0.010789560585799274, 0.005574623833501143, 0.005378430792866547, 0.008639148297115318, 0.003640533513350533, 0.005241122456721162, 0.00439345651692048, 0.0083198784017303, 0.005883702423043503, 0.008560184770999733, 0.005396376183622979, 0.009114075717667938, 0.006958998821409742, 0.016849992899624907, 0.009020700448318398, 0.013718106625922516, 0.010545307852358549, 0.012933817469909091]
scaleW
 [0.17907369959686095, 0.17082252386187305, 0.21059261511088623, 0.13659665702352078, 0.14849466288026858, 0.08965018041381462, 0.10026675033774607, 0.08756479406046586, 0.07544499182368548, 0.05223211702124018, 0.07950400180282145, 0.04952566733119041, 0.05192139025115724, 0.04631257760089999, 0.03863659455284512, 0.03076435788703427, 0.038518756117883864, 0.020054703974230186]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 060 	 Test accuracy: 74.11 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 061 	 Test accuracy: 59.38 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 062 	 Test accuracy: 56.97 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 063 	 Test accuracy: 55.26 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 064 	 Test accuracy: 66.99000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 065 	 Test accuracy: 53.690000000000005 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 066 	 Test accuracy: 60.980000000000004 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 067 	 Test accuracy: 70.11 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 068 	 Test accuracy: 52.1 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 069 	 Test accuracy: 78.49000000000001 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:22<02:45, 82.65s/it] 67%|   | 2/3 [02:41<01:20, 80.61s/it]100%|| 3/3 [04:11<00:00, 84.94s/it]100%|| 3/3 [04:11<00:00, 83.99s/it]


scaleA
 [0.010361228331807637, 0.005656800865917276, 0.004854602962443447, 0.008361475664202365, 0.004435173822381419, 0.005513224954586518, 0.003768157546856086, 0.006896451512931215, 0.005261018683355084, 0.007285658423157528, 0.004488344131726822, 0.007577962023771599, 0.006167989259981168, 0.015467612069355203, 0.0065035749316429995, 0.010312414738279096, 0.008365807205096573, 0.010826752572976892]
scaleW
 [0.17055761848986006, 0.1573443806914858, 0.2157995139384545, 0.1266898919155046, 0.16686228420626684, 0.08376389933320288, 0.09416822120919853, 0.06976614424218547, 0.07181639710831175, 0.043907009028316075, 0.07046541265100591, 0.04036027656287847, 0.04441483490048548, 0.037446204526991274, 0.027351708619842336, 0.022525525242665805, 0.031419550553057715, 0.016195857543881847]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 070 	 Test accuracy: 63.43 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 071 	 Test accuracy: 67.75999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 072 	 Test accuracy: 71.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 073 	 Test accuracy: 60.099999999999994 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 074 	 Test accuracy: 53.28000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 075 	 Test accuracy: 71.56 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 076 	 Test accuracy: 65.59 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 077 	 Test accuracy: 68.57 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 078 	 Test accuracy: 58.220000000000006 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 079 	 Test accuracy: 63.06 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:29<02:58, 89.17s/it] 67%|   | 2/3 [02:37<01:16, 76.90s/it]100%|| 3/3 [03:50<00:00, 75.09s/it]100%|| 3/3 [03:50<00:00, 76.82s/it]


scaleA
 [0.011649008263722976, 0.006101880833733882, 0.0057262600285523544, 0.008599130996240885, 0.003899716704303143, 0.005876459835023135, 0.005037085568613649, 0.00968878316284433, 0.00568779134012773, 0.00867508379321768, 0.004815868123431144, 0.009193219678911147, 0.0073661692062121364, 0.01841707962902608, 0.008156671283050568, 0.013344066642561998, 0.009694679927595568, 0.011219765129842298]
scaleW
 [0.19805062295966916, 0.17228243928725895, 0.21165961168237823, 0.15349550370643605, 0.18265713607523737, 0.0891804833200932, 0.12018572589803707, 0.09308295585424757, 0.08274314073543504, 0.0545738681564116, 0.0704839320351237, 0.04436946701242094, 0.05490912370518081, 0.04955263257447335, 0.036104230321763596, 0.03054785072975971, 0.035464427321404556, 0.017002346307572025]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 080 	 Test accuracy: 68.52000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 081 	 Test accuracy: 74.83999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 082 	 Test accuracy: 60.6 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 083 	 Test accuracy: 62.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 084 	 Test accuracy: 62.160000000000004 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 085 	 Test accuracy: 65.98 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 086 	 Test accuracy: 68.28999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 087 	 Test accuracy: 71.78 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 088 	 Test accuracy: 46.19 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 089 	 Test accuracy: 65.25999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:06<02:13, 66.71s/it] 67%|   | 2/3 [02:16<01:08, 68.55s/it]100%|| 3/3 [03:23<00:00, 67.74s/it]100%|| 3/3 [03:23<00:00, 67.78s/it]


scaleA
 [0.011383475023546983, 0.005816441420944635, 0.006204112025460125, 0.008800610124966711, 0.0038462017750960434, 0.0056530922109699865, 0.004305800446239484, 0.007981226051911332, 0.005287254991917581, 0.008123635535031183, 0.005118575921569951, 0.011089914015753953, 0.006685065358389956, 0.015424403627494816, 0.007874138977615158, 0.012795645872835394, 0.009200242882113907, 0.01220924031977326]
scaleW
 [0.19387249422462696, 0.17933171799944528, 0.24518068327895293, 0.1464222887887531, 0.1913629181686798, 0.09163429066262622, 0.09768698513391633, 0.08045087677804597, 0.07325190357317296, 0.05267826926293523, 0.07773580819884388, 0.055218171994438826, 0.04975886487259553, 0.039256376388289334, 0.032996291130175416, 0.02426740056699263, 0.03464900467389496, 0.018232885638381123]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 090 	 Test accuracy: 54.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 091 	 Test accuracy: 53.7 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 092 	 Test accuracy: 56.13 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 093 	 Test accuracy: 45.379999999999995 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 094 	 Test accuracy: 64.18 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 095 	 Test accuracy: 75.2 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 096 	 Test accuracy: 75.78 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 097 	 Test accuracy: 76.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 098 	 Test accuracy: 75.86 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 099 	 Test accuracy: 71.82 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:58<01:57, 58.92s/it] 67%|   | 2/3 [02:03<01:02, 62.33s/it]100%|| 3/3 [03:08<00:00, 63.36s/it]100%|| 3/3 [03:08<00:00, 62.75s/it]


scaleA
 [0.01224017809566113, 0.006566143121396806, 0.0065270747244450976, 0.01043936257863544, 0.005592442061426465, 0.0063734709748750765, 0.004788509082078463, 0.008795643602104653, 0.005767772586169118, 0.008811348978906295, 0.006529342519834463, 0.011008285745822395, 0.00822457484428276, 0.019726388945094684, 0.008499271283096658, 0.014797759830805954, 0.009400568999873819, 0.013030830903897417]
scaleW
 [0.21771142016252876, 0.2175369453777988, 0.28486294892230457, 0.19075112291369878, 0.22349831552607066, 0.10254862931697621, 0.12109581912790122, 0.09327455987910749, 0.08019991530511351, 0.05030223366756656, 0.1021868758572827, 0.05552852155848842, 0.05543023299833896, 0.0520576905104388, 0.03572875293702744, 0.02875279610142679, 0.03208183613524198, 0.015959895926187056]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 100 	 Test accuracy: 65.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 101 	 Test accuracy: 75.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 102 	 Test accuracy: 60.27 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 103 	 Test accuracy: 70.67999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 104 	 Test accuracy: 72.6 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 105 	 Test accuracy: 60.919999999999995 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 106 	 Test accuracy: 71.65 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 107 	 Test accuracy: 77.48 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 108 	 Test accuracy: 76.18 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 109 	 Test accuracy: 75.68 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:23<02:47, 83.85s/it] 67%|   | 2/3 [02:49<01:24, 84.72s/it]100%|| 3/3 [03:50<00:00, 74.18s/it]100%|| 3/3 [03:50<00:00, 76.95s/it]


scaleA
 [0.010992375075904988, 0.006550846802695975, 0.005517758002496401, 0.009237253747235961, 0.004048516293252888, 0.005294015004283818, 0.0042289004931014154, 0.007921648409301532, 0.006258698094605932, 0.0076475363531662175, 0.005167080007659396, 0.009246120598405976, 0.007524805689873963, 0.01758050013737132, 0.005950970003286739, 0.011814528089605479, 0.007104706272911414, 0.010099935322791105]
scaleW
 [0.18239536760683916, 0.18295838467443867, 0.23509632505646985, 0.1683428241077709, 0.16663440911450206, 0.08222052763257966, 0.10422178500909551, 0.07963580282585392, 0.08172989764616519, 0.04968018486550302, 0.07790509115264437, 0.04157483054089928, 0.05875943030163174, 0.0461413036691537, 0.026526803970813473, 0.022030157670171458, 0.029309820581616287, 0.013019031221400563]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 110 	 Test accuracy: 72.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 111 	 Test accuracy: 78.88 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 112 	 Test accuracy: 57.66 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 113 	 Test accuracy: 65.12 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 114 	 Test accuracy: 55.2 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 115 	 Test accuracy: 50.28 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 116 	 Test accuracy: 68.84 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 117 	 Test accuracy: 68.07 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 118 	 Test accuracy: 69.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 119 	 Test accuracy: 74.68 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:58<01:56, 58.44s/it] 67%|   | 2/3 [02:12<01:07, 67.56s/it]100%|| 3/3 [03:34<00:00, 74.36s/it]100%|| 3/3 [03:34<00:00, 71.62s/it]


scaleA
 [0.009866734219325109, 0.005578143545837209, 0.005349382888858172, 0.008030600145239939, 0.0040092902466269, 0.005721325356493504, 0.0038201972924489687, 0.007661356227927139, 0.004832536225580129, 0.00680572923183058, 0.004716640703662937, 0.007508799792862065, 0.006592608770020495, 0.017292186600635466, 0.0072142494123803, 0.012512233388662293, 0.008185609741550297, 0.010720035056843732]
scaleW
 [0.17755535097470512, 0.15696402687949298, 0.1841231447300169, 0.1443276755694057, 0.1457250088725298, 0.08439938006230215, 0.1007655099320457, 0.07952158358355266, 0.0642037104437848, 0.042457985152852096, 0.07715065148180292, 0.036037411649436996, 0.04730198082759049, 0.04157252196228646, 0.03430931267848198, 0.02369039538757704, 0.02908508953364275, 0.011215270645428257]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 120 	 Test accuracy: 61.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 121 	 Test accuracy: 73.45 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 122 	 Test accuracy: 78.18 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 123 	 Test accuracy: 42.57 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 124 	 Test accuracy: 59.199999999999996 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 125 	 Test accuracy: 49.62 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 126 	 Test accuracy: 73.11999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 127 	 Test accuracy: 70.52000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 128 	 Test accuracy: 76.36 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 129 	 Test accuracy: 73.79 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:21<02:42, 81.49s/it] 67%|   | 2/3 [03:00<01:32, 92.07s/it]100%|| 3/3 [04:26<00:00, 88.97s/it]100%|| 3/3 [04:26<00:00, 88.76s/it]


scaleA
 [0.012252532242178213, 0.00625247192985653, 0.006475744708491725, 0.009582460529333313, 0.00460569168702936, 0.005985592545944583, 0.004271589719670903, 0.008388507750526108, 0.005435449462689722, 0.007918860312939029, 0.006069315616065615, 0.00937827841455721, 0.0070510374260088085, 0.017860491169294095, 0.0075558938103769055, 0.015697136753936608, 0.010251267001707783, 0.014617363559583893]
scaleW
 [0.21166597179193422, 0.18381112111052714, 0.23213419716367203, 0.16563023657446144, 0.19605741537319, 0.1030215266112789, 0.10796736699145036, 0.08666030869023385, 0.07247507279441413, 0.04221220914591526, 0.09252992618146953, 0.04074465891859808, 0.05126162657587178, 0.04424569583451213, 0.036776131421332375, 0.03165392548283886, 0.038810276493047254, 0.020084737960004487]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 130 	 Test accuracy: 78.25999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 131 	 Test accuracy: 65.28 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 132 	 Test accuracy: 64.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 133 	 Test accuracy: 81.96 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 134 	 Test accuracy: 71.81 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 135 	 Test accuracy: 67.11 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 136 	 Test accuracy: 70.95 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 137 	 Test accuracy: 62.4 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 138 	 Test accuracy: 63.32 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 139 	 Test accuracy: 66.56 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:13<02:26, 73.43s/it] 67%|   | 2/3 [02:24<01:11, 71.88s/it]100%|| 3/3 [03:37<00:00, 72.61s/it]100%|| 3/3 [03:37<00:00, 72.58s/it]


scaleA
 [0.010888755356033087, 0.0056011518306624125, 0.005958870007178385, 0.008558159940773791, 0.0039028731017460953, 0.005545476706063715, 0.004121959893772204, 0.007595038908469377, 0.005201925353509479, 0.007086444396414117, 0.005320410603855116, 0.010170748923951445, 0.0060138113405449264, 0.015798934321336087, 0.00695795014009104, 0.01430978597455064, 0.0090808981544707, 0.012463947034398124]
scaleW
 [0.1937342070895481, 0.17877148657125588, 0.24780566969732157, 0.15563837568028718, 0.1845554032195131, 0.10030169879022958, 0.10709269700918195, 0.07758350873692157, 0.0741640812919026, 0.04397355520372928, 0.08044398184274396, 0.05289642626145253, 0.04418800381143009, 0.04035900323621636, 0.03172802150163021, 0.029388279475070656, 0.034887692066238866, 0.01505119197947407]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 140 	 Test accuracy: 69.57 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 141 	 Test accuracy: 78.34 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 142 	 Test accuracy: 72.07000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 143 	 Test accuracy: 75.48 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 144 	 Test accuracy: 78.02 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 145 	 Test accuracy: 74.22999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 146 	 Test accuracy: 71.41999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 147 	 Test accuracy: 74.9 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 148 	 Test accuracy: 71.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 149 	 Test accuracy: 76.68 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:12<02:24, 72.18s/it] 67%|   | 2/3 [02:38<01:20, 80.54s/it]100%|| 3/3 [03:48<00:00, 75.51s/it]100%|| 3/3 [03:48<00:00, 76.04s/it]


scaleA
 [0.013206602468242833, 0.006495527625212982, 0.006573902901418428, 0.009020851872405877, 0.004311771551430855, 0.006651964984193755, 0.004655551527202334, 0.008939927835993821, 0.006711918529707769, 0.008504956829178564, 0.005498871434235021, 0.01014980740171093, 0.006995794766878015, 0.017721197176361485, 0.007093726186737361, 0.013283777266428085, 0.011266116911702852, 0.014471431568302643]
scaleW
 [0.22218802368823795, 0.20745725315265784, 0.26269231208779936, 0.14609787437473887, 0.1719947311581511, 0.11720427994718426, 0.10800575517783413, 0.09083720388282031, 0.09055179506358207, 0.05537280675816083, 0.08315526583956089, 0.05091687420616767, 0.05347627325597832, 0.048116192418521046, 0.03584029373571435, 0.02724995861953411, 0.041878637936697265, 0.017727336690960535]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 150 	 Test accuracy: 66.95 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 151 	 Test accuracy: 73.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 152 	 Test accuracy: 68.5 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 153 	 Test accuracy: 79.96 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 154 	 Test accuracy: 79.93 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 155 	 Test accuracy: 74.92 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 156 	 Test accuracy: 72.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 157 	 Test accuracy: 65.24 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 158 	 Test accuracy: 68.76 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 159 	 Test accuracy: 78.97999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:23<02:46, 83.29s/it] 67%|   | 2/3 [02:53<01:27, 87.47s/it]100%|| 3/3 [04:02<00:00, 79.12s/it]100%|| 3/3 [04:02<00:00, 80.97s/it]


scaleA
 [0.01197936460021254, 0.006071213353362688, 0.006706291694818321, 0.009512435499214069, 0.00466721389119381, 0.005911189211545318, 0.004875515938502352, 0.00857431772557806, 0.004463473292145381, 0.005998989170292287, 0.005420297886302556, 0.010611777807884892, 0.007070332881864941, 0.017096280384991745, 0.007931320670628857, 0.015388025007385972, 0.012257167405849182, 0.015692056026387628]
scaleW
 [0.19019155387426523, 0.18649767233447678, 0.2573451973155212, 0.17274194760025333, 0.18709022960854785, 0.09883396278725015, 0.11378714111749848, 0.0891959526712129, 0.06317572407478962, 0.03329004661026024, 0.08624567197380334, 0.05550841006336522, 0.05380333564439089, 0.0434907505414914, 0.03657197988044796, 0.03175606304480998, 0.04217425980947531, 0.018747265662432316]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 160 	 Test accuracy: 82.28 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 161 	 Test accuracy: 75.53 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 162 	 Test accuracy: 77.27000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 163 	 Test accuracy: 79.03 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 164 	 Test accuracy: 72.45 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 165 	 Test accuracy: 74.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 166 	 Test accuracy: 80.05 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 167 	 Test accuracy: 73.22999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 168 	 Test accuracy: 73.82 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 169 	 Test accuracy: 75.13 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:10<02:20, 70.03s/it] 67%|   | 2/3 [02:20<01:10, 70.25s/it]100%|| 3/3 [03:51<00:00, 79.94s/it]100%|| 3/3 [03:51<00:00, 77.31s/it]


scaleA
 [0.013037468204797809, 0.00645592495175538, 0.006304419708688194, 0.008908912923500238, 0.004708214549716958, 0.006068357942426817, 0.004870949045623858, 0.009162538889352667, 0.005896399605739806, 0.008179879314466629, 0.004757554167266292, 0.00949723709767883, 0.006894402410452617, 0.01705978338826802, 0.008048734294363464, 0.01717942976095482, 0.009383376318893223, 0.015736298033557635]
scaleW
 [0.21130507800235335, 0.18342211457013233, 0.24504698550068416, 0.1562631047197021, 0.19029769357142504, 0.10580600986168914, 0.11249718110745593, 0.09218066422139526, 0.07708002814693635, 0.05308059581587828, 0.07375168475194338, 0.04857139485629878, 0.052497397282045555, 0.04312934984974551, 0.03996119186622558, 0.03420922232240683, 0.03597772757375036, 0.017596451319020486]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 170 	 Test accuracy: 77.24 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 171 	 Test accuracy: 74.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 172 	 Test accuracy: 85.21 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 173 	 Test accuracy: 72.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 174 	 Test accuracy: 66.3 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 175 	 Test accuracy: 66.84 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 176 	 Test accuracy: 69.47 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 177 	 Test accuracy: 74.2 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 178 	 Test accuracy: 73.38 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 179 	 Test accuracy: 78.25 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [00:56<01:52, 56.11s/it] 67%|   | 2/3 [02:13<01:08, 68.67s/it]100%|| 3/3 [03:23<00:00, 69.30s/it]100%|| 3/3 [03:23<00:00, 67.89s/it]


scaleA
 [0.013201645632319375, 0.006947904214832698, 0.006782029161495163, 0.011034184878032885, 0.005185575355680418, 0.00671570308409426, 0.005332807619017883, 0.00982810009439881, 0.007110679159097477, 0.008146414986060705, 0.0066712509165611665, 0.009830037974347302, 0.007993476906823624, 0.019620413400555026, 0.00929520787319045, 0.020177522324772972, 0.013843712495313387, 0.018808850297119813]
scaleW
 [0.23231247693371546, 0.219285401913037, 0.2859858743909351, 0.19518854800365001, 0.2213803415027499, 0.12296387794165335, 0.1307883165356561, 0.10081476752771647, 0.09644977079969634, 0.05219245346438228, 0.10141284248245837, 0.05537001460789232, 0.06358486486629825, 0.055047237874137754, 0.046881231676944246, 0.0405561833229529, 0.056108083446470614, 0.02223087764112949]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 180 	 Test accuracy: 77.48 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 181 	 Test accuracy: 75.44 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 182 	 Test accuracy: 73.97 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 183 	 Test accuracy: 85.37 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 184 	 Test accuracy: 60.419999999999995 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 185 	 Test accuracy: 79.58 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 186 	 Test accuracy: 75.5 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 187 	 Test accuracy: 70.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 188 	 Test accuracy: 70.08 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 189 	 Test accuracy: 80.34 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:26<02:52, 86.39s/it] 67%|   | 2/3 [02:51<01:25, 85.81s/it]100%|| 3/3 [03:55<00:00, 75.54s/it]100%|| 3/3 [03:55<00:00, 78.38s/it]


scaleA
 [0.011528706598386418, 0.006095079651664795, 0.006568848803781914, 0.01001929012862108, 0.004351575427485731, 0.005978476313862294, 0.004203610767940888, 0.007986085700022988, 0.005034467538686919, 0.007001903990473398, 0.005150116062835796, 0.008619891603545387, 0.00639934701958032, 0.016183311148963875, 0.007218623292289619, 0.012271375454745952, 0.011153521275429477, 0.0179325286248347]
scaleW
 [0.19106319718261602, 0.17578764337367483, 0.27007579360217293, 0.1934998117381829, 0.17507495968316159, 0.10825942693056907, 0.1063670675777853, 0.0851225139599167, 0.06850062227301497, 0.04393654266897596, 0.07807885132737545, 0.04629390720389454, 0.05631814039792802, 0.04353023044245654, 0.03553149620471008, 0.02546722152171005, 0.042135439169056856, 0.023102904016408233]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 190 	 Test accuracy: 76.81 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 191 	 Test accuracy: 80.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 192 	 Test accuracy: 83.24000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 193 	 Test accuracy: 75.61 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 194 	 Test accuracy: 74.22999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 195 	 Test accuracy: 83.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 196 	 Test accuracy: 85.44 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 197 	 Test accuracy: 82.03 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 198 	 Test accuracy: 84.11999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 199 	 Test accuracy: 84.96000000000001 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:26<02:52, 86.14s/it] 67%|   | 2/3 [02:33<01:14, 74.84s/it]100%|| 3/3 [03:57<00:00, 79.09s/it]100%|| 3/3 [03:57<00:00, 79.08s/it]


scaleA
 [0.012066230430952932, 0.0060920766773804496, 0.005882263296509169, 0.00922821952288559, 0.004536750971405022, 0.006105102917785358, 0.004573978233736777, 0.008384417832046335, 0.005912937711545277, 0.00899048109342003, 0.004746604815973635, 0.009753367449785938, 0.00679080544833869, 0.016556395978727018, 0.0067091976501064965, 0.018026960182635266, 0.012677324884951519, 0.02024656303452385]
scaleW
 [0.20288552782962124, 0.19870418016089916, 0.25492988713201953, 0.1747485694020464, 0.21413847347944673, 0.11680237830651573, 0.13024831432391287, 0.09463903727479343, 0.08906732153412349, 0.061161195991074946, 0.08710890872954398, 0.051727768273018705, 0.05896800224130443, 0.048258111818166784, 0.03634981972783, 0.037943466016295245, 0.052284829061928, 0.025221545525847874]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 200 	 Test accuracy: 84.27 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 201 	 Test accuracy: 84.07 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 202 	 Test accuracy: 81.33 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 203 	 Test accuracy: 75.86 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 204 	 Test accuracy: 77.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 205 	 Test accuracy: 83.89999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 206 	 Test accuracy: 65.75999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 207 	 Test accuracy: 82.94 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 208 	 Test accuracy: 85.14 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 209 	 Test accuracy: 85.28999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:19<02:38, 79.37s/it] 67%|   | 2/3 [02:21<01:09, 69.37s/it]100%|| 3/3 [03:25<00:00, 66.70s/it]100%|| 3/3 [03:25<00:00, 68.43s/it]


scaleA
 [0.01253180058074581, 0.006127957601716573, 0.0064891368769978185, 0.01012976476577847, 0.005696182947820714, 0.006239246688262705, 0.0040057932430964446, 0.00795761950188501, 0.006684241422336065, 0.00815930159405795, 0.006200894004615774, 0.010484148178471793, 0.006708535369279352, 0.016802242821805733, 0.006589252575667203, 0.014752810606035643, 0.01001430276845846, 0.01641576563941967]
scaleW
 [0.2195979242024825, 0.20016944968621053, 0.27363634974373613, 0.21995110740830506, 0.22073606336161175, 0.13822881726440392, 0.11058710260824904, 0.08655701385643187, 0.08424287350900371, 0.05414546900957823, 0.09260635894295442, 0.05521592933727526, 0.059286644931385, 0.04639177491169636, 0.033403197535724446, 0.03021110815215941, 0.042343052429844735, 0.01935255561513004]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 210 	 Test accuracy: 69.77 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 211 	 Test accuracy: 84.93 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 212 	 Test accuracy: 83.31 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 213 	 Test accuracy: 70.88 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 214 	 Test accuracy: 87.01 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 215 	 Test accuracy: 80.62 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 216 	 Test accuracy: 86.03 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 217 	 Test accuracy: 82.17999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 218 	 Test accuracy: 84.61999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 219 	 Test accuracy: 82.74000000000001 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:25<02:50, 85.02s/it] 67%|   | 2/3 [02:32<01:14, 74.88s/it]100%|| 3/3 [03:40<00:00, 71.57s/it]100%|| 3/3 [03:40<00:00, 73.49s/it]


scaleA
 [0.013752099936752138, 0.007125866614014885, 0.007278012180665142, 0.010900139261763057, 0.006223850640116081, 0.0075211502866572785, 0.005642477457415164, 0.009533889580290717, 0.006301914575406462, 0.008402225081822992, 0.005685696336597306, 0.007885177681824831, 0.007004377958644154, 0.017618901892800312, 0.008438328832233239, 0.020464678951358655, 0.01343435014858657, 0.02067650203885061]
scaleW
 [0.24961086928605927, 0.21425354862596568, 0.3033982792196415, 0.2173144714614675, 0.2676529736028595, 0.13804894545891916, 0.14351442466502642, 0.09915280909905004, 0.08568439170432253, 0.053969454309574005, 0.08919832507438459, 0.03869303081160861, 0.058803315210487504, 0.04907371154325998, 0.04368028010402847, 0.04054911331309229, 0.053694538011717176, 0.025777288018298403]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 220 	 Test accuracy: 85.16 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 221 	 Test accuracy: 77.78 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 222 	 Test accuracy: 83.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 223 	 Test accuracy: 76.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 224 	 Test accuracy: 84.54 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 225 	 Test accuracy: 81.39999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 226 	 Test accuracy: 75.3 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 227 	 Test accuracy: 83.17 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 228 	 Test accuracy: 78.73 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 229 	 Test accuracy: 85.2 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:27<02:54, 87.07s/it] 67%|   | 2/3 [02:46<01:22, 82.31s/it]100%|| 3/3 [04:18<00:00, 86.96s/it]100%|| 3/3 [04:18<00:00, 86.19s/it]


scaleA
 [0.012308226007611398, 0.005999017267622554, 0.00626022708096581, 0.008918630880713619, 0.004638723551259719, 0.0058045119972813435, 0.00447366875905219, 0.007753690213822262, 0.004902254590539788, 0.00693849378932953, 0.005038297355117439, 0.006250441103703708, 0.0062230312909304615, 0.015852375025343857, 0.007307687088590624, 0.01543681469011586, 0.011239868083817732, 0.018123683884113135]
scaleW
 [0.21418300984285918, 0.19127401211728157, 0.25857940358212933, 0.16646145940109613, 0.17471443155748131, 0.09684358763820049, 0.11049033786854873, 0.08717734812470927, 0.07018725113210562, 0.04591373806546418, 0.07788610588711718, 0.028302102903196725, 0.05334624309564739, 0.04229391640702015, 0.03810560008757213, 0.029856586778786044, 0.04463056263655959, 0.022426255321251403]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 230 	 Test accuracy: 81.78 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 231 	 Test accuracy: 84.74000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 232 	 Test accuracy: 85.44 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 233 	 Test accuracy: 84.16 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 234 	 Test accuracy: 78.41 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 235 	 Test accuracy: 81.31 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 236 	 Test accuracy: 83.73 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 237 	 Test accuracy: 85.72999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 238 	 Test accuracy: 85.8 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 239 	 Test accuracy: 82.07 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:12<02:25, 72.64s/it] 67%|   | 2/3 [02:31<01:16, 76.25s/it]100%|| 3/3 [04:03<00:00, 83.28s/it]100%|| 3/3 [04:03<00:00, 81.03s/it]


scaleA
 [0.013865150744221247, 0.006330575909810375, 0.006932135565727574, 0.00918514778395576, 0.005344298719814357, 0.006842063453502, 0.0048340514218689155, 0.008513537149563141, 0.005909015119972945, 0.008910369556833492, 0.005890564999111183, 0.01072261198562024, 0.008233784813270006, 0.020627140387447902, 0.008872477609429719, 0.01732048871456509, 0.012822738209767598, 0.01983739287479316]
scaleW
 [0.2390494840528131, 0.21402318694830472, 0.2857119050660338, 0.19062820924057877, 0.2514143701157425, 0.13422633856959165, 0.13399531294471514, 0.10534660843074806, 0.09410460293955525, 0.06471587496314314, 0.10517242220162064, 0.06502386694193651, 0.07018121932181344, 0.0607853753685774, 0.0504638108420209, 0.03467781886749657, 0.05366048442674825, 0.02452257339159976]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 191
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 240 	 Test accuracy: 81.52000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 241 	 Test accuracy: 84.91 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 242 	 Test accuracy: 85.96000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 243 	 Test accuracy: 74.17 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 244 	 Test accuracy: 82.8 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 245 	 Test accuracy: 79.41 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 246 	 Test accuracy: 82.32000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 247 	 Test accuracy: 83.52000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 248 	 Test accuracy: 80.9 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 249 	 Test accuracy: 79.14999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:38<03:17, 98.67s/it] 67%|   | 2/3 [03:00<01:28, 88.92s/it]100%|| 3/3 [03:51<00:00, 71.57s/it]100%|| 3/3 [03:51<00:00, 77.24s/it]


scaleA
 [0.013369727022946096, 0.006152857631498409, 0.006660424957684964, 0.010304964330983123, 0.005249202765382206, 0.0072750955072814994, 0.005231680504020728, 0.008901979144570003, 0.006843340107507749, 0.009225737511666506, 0.00650329219857471, 0.0094924092897889, 0.007246509673835732, 0.01697440025353621, 0.006988282143481318, 0.015112100244986534, 0.010846970138190545, 0.020426016169566616]
scaleW
 [0.20378092770690504, 0.20587251132119452, 0.301104790011257, 0.20349094432431322, 0.24513272408905973, 0.15070617566255914, 0.13090656121736888, 0.1018542041028321, 0.09868396579688561, 0.06840498472867297, 0.10973753402229802, 0.055310644355027604, 0.06380182272318426, 0.046086808619907084, 0.03912648471997058, 0.03167864021682735, 0.04717179864269779, 0.02577863147691804]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 250 	 Test accuracy: 84.26 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 251 	 Test accuracy: 87.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 252 	 Test accuracy: 74.39 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 253 	 Test accuracy: 84.92 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 254 	 Test accuracy: 84.0 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 255 	 Test accuracy: 86.99 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 256 	 Test accuracy: 82.19999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 257 	 Test accuracy: 82.26 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 258 	 Test accuracy: 86.21 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 259 	 Test accuracy: 81.15 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:29<02:58, 89.50s/it] 67%|   | 2/3 [02:34<01:15, 75.14s/it]100%|| 3/3 [03:46<00:00, 73.69s/it]100%|| 3/3 [03:46<00:00, 75.54s/it]


scaleA
 [0.015350724762019638, 0.007532314046090623, 0.009054798106511296, 0.011050938385717307, 0.006816865466265112, 0.008107073518649309, 0.00529839142044362, 0.00952658020568048, 0.00706084926308425, 0.008234332285256043, 0.00544283365985702, 0.009456415998043017, 0.008266760712095957, 0.021663435418524557, 0.009755377128111703, 0.020927499772786313, 0.013703894183647298, 0.021566450020050736]
scaleW
 [0.26980988307977094, 0.24998735279263917, 0.3637880633996045, 0.2622152513842849, 0.2664118207543544, 0.1499219507932655, 0.15406989551475495, 0.10987565197851006, 0.10389773758398757, 0.06304009106864922, 0.10656316947839689, 0.06078745294766835, 0.07252029933016271, 0.0602454652708258, 0.054885602109519004, 0.046051418931218925, 0.06320523197056306, 0.026265118662617732]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 260 	 Test accuracy: 82.80999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 261 	 Test accuracy: 86.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 262 	 Test accuracy: 79.63 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 263 	 Test accuracy: 86.91 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 264 	 Test accuracy: 84.66 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 265 	 Test accuracy: 86.46000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 266 	 Test accuracy: 85.97 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 267 	 Test accuracy: 83.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 268 	 Test accuracy: 82.38 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 269 	 Test accuracy: 85.89 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:24<02:48, 84.39s/it] 67%|   | 2/3 [03:03<01:33, 93.25s/it]100%|| 3/3 [04:28<00:00, 89.39s/it]100%|| 3/3 [04:28<00:00, 89.56s/it]


scaleA
 [0.01143962512618972, 0.005417513347277496, 0.006203023091198134, 0.008502360633232895, 0.004914864228871347, 0.005611841090454946, 0.004483095022097734, 0.008531832771707305, 0.005053738352753421, 0.007465710219175873, 0.00558570936915627, 0.0087036769208975, 0.007015756937506011, 0.017525073010380175, 0.007253014921575929, 0.016545210585448004, 0.01208839796945294, 0.02125551393924772]
scaleW
 [0.21064792379932676, 0.1923854258573333, 0.25466816017855404, 0.18063727017592643, 0.20585979210456617, 0.1078434717810719, 0.11595048470040425, 0.10513442690076452, 0.08686319947297753, 0.05835229038640897, 0.10286539782575978, 0.059864519114938376, 0.06243088570580583, 0.051081056000233806, 0.04122589894071794, 0.03930167064480469, 0.05510888070513273, 0.025233971252229048]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 270 	 Test accuracy: 87.29 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 271 	 Test accuracy: 78.84 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 272 	 Test accuracy: 82.89 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 273 	 Test accuracy: 85.06 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 274 	 Test accuracy: 86.16 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 275 	 Test accuracy: 86.86 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 276 	 Test accuracy: 87.51 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 277 	 Test accuracy: 87.2 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 278 	 Test accuracy: 83.59 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 279 	 Test accuracy: 83.67 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:26<02:52, 86.20s/it] 67%|   | 2/3 [02:43<01:21, 81.13s/it]100%|| 3/3 [03:45<00:00, 72.33s/it]100%|| 3/3 [03:45<00:00, 75.22s/it]


scaleA
 [0.017117968031627566, 0.007788190192973432, 0.009671780797008528, 0.011604818433762433, 0.00679210261643163, 0.007875106188452328, 0.0062900797586554905, 0.010302690169930656, 0.006967375773786924, 0.009119219973808252, 0.00661903910750064, 0.010861280206925825, 0.009148055916070564, 0.022563851903518617, 0.008696088038044563, 0.01884751863986979, 0.012873410547774229, 0.023693232501884206]
scaleW
 [0.2850101493732456, 0.27856815810032926, 0.4201742758749229, 0.24283430029510256, 0.2982265423797876, 0.16066470894005627, 0.16556610994612944, 0.12829242846947644, 0.11331235883211643, 0.07630555631854673, 0.12500079037718834, 0.07952432270738069, 0.08075923248113244, 0.07228761850778458, 0.053266769372814604, 0.0448307328675878, 0.05972816870627964, 0.030320663999046423]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 280 	 Test accuracy: 87.09 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 281 	 Test accuracy: 86.7 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 282 	 Test accuracy: 86.98 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 283 	 Test accuracy: 85.28999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 284 	 Test accuracy: 87.71 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 285 	 Test accuracy: 87.36 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 286 	 Test accuracy: 87.05000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 287 	 Test accuracy: 86.59 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 288 	 Test accuracy: 86.41 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 289 	 Test accuracy: 85.52 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:39<03:19, 99.59s/it] 67%|   | 2/3 [02:56<01:26, 86.16s/it]100%|| 3/3 [04:22<00:00, 86.34s/it]100%|| 3/3 [04:22<00:00, 87.65s/it]


scaleA
 [0.014159679353776456, 0.006656984464378083, 0.006388536619659692, 0.009959730619901986, 0.006175934302055509, 0.006568394798453528, 0.005222127178080547, 0.009306729703687821, 0.0074998856238141925, 0.010246809973098165, 0.006530192557049487, 0.010734166575157857, 0.008083165242920156, 0.019729141564814767, 0.00819208640392557, 0.017701399506887424, 0.011452506252637265, 0.01902596545050648]
scaleW
 [0.30481475723631957, 0.2706006350578298, 0.3004701479259026, 0.21308562455211363, 0.2772219179830235, 0.13405000233318065, 0.1556353645398859, 0.1055800043917254, 0.1162644966053062, 0.09060292126884152, 0.11177311572450893, 0.0804408464441726, 0.0754597317632555, 0.06206475127384459, 0.05312934886539631, 0.046413009893209504, 0.05805410509837011, 0.024304565978813992]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 290 	 Test accuracy: 86.88 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 291 	 Test accuracy: 86.17 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 292 	 Test accuracy: 84.88 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 293 	 Test accuracy: 84.19 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 294 	 Test accuracy: 85.04 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 295 	 Test accuracy: 87.29 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 296 	 Test accuracy: 87.62 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 297 	 Test accuracy: 83.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 298 	 Test accuracy: 83.57 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 299 	 Test accuracy: 87.14 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:36<03:12, 96.42s/it] 67%|   | 2/3 [02:56<01:26, 86.93s/it]100%|| 3/3 [04:18<00:00, 84.41s/it]100%|| 3/3 [04:18<00:00, 86.05s/it]


scaleA
 [0.012884853052186321, 0.0057904066321173295, 0.006747387931746043, 0.009142258735393588, 0.004859656208002766, 0.00593202581068743, 0.0048678789438081314, 0.007842886772030011, 0.0057931439657816055, 0.007564234877911551, 0.005271433415372193, 0.008237248532941622, 0.0067056692420687175, 0.018953898273605062, 0.008866695265597415, 0.022944632966078418, 0.011507920623093797, 0.021610181239972798]
scaleW
 [0.2882947176909379, 0.22021815229800637, 0.316990306724735, 0.23470435951086643, 0.22842332242044994, 0.11666274516964288, 0.1270659333203625, 0.09733616165968345, 0.08988600042339379, 0.058589155271958925, 0.0945072693575646, 0.06551564252276076, 0.061198711817863016, 0.05827192776112452, 0.05841443202224992, 0.06080796526143926, 0.05231933297963815, 0.02739093595349383]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 300 	 Test accuracy: 88.02 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 301 	 Test accuracy: 86.29 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 302 	 Test accuracy: 86.33 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 303 	 Test accuracy: 86.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 304 	 Test accuracy: 87.25 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 305 	 Test accuracy: 88.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 306 	 Test accuracy: 87.0 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 307 	 Test accuracy: 87.57000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 308 	 Test accuracy: 86.16 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 309 	 Test accuracy: 87.91 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:10<02:21, 70.74s/it] 67%|   | 2/3 [02:34<01:18, 78.31s/it]100%|| 3/3 [03:44<00:00, 74.65s/it]100%|| 3/3 [03:44<00:00, 74.89s/it]


scaleA
 [0.014535991868996928, 0.006638460708944757, 0.007273398810269939, 0.010199312814311983, 0.0057187899562445715, 0.006303625553001278, 0.005096862943696473, 0.008698752983778616, 0.007131260879151286, 0.009825618142019361, 0.006407575156970462, 0.011540063243669125, 0.007114654969640057, 0.018313682537107603, 0.008103786843759724, 0.019451141808230845, 0.011956168783682242, 0.018695763444266236]
scaleW
 [0.25574915201947795, 0.23258704035233155, 0.34960721001178, 0.22803204885434972, 0.23249095852624638, 0.14793300614579763, 0.16000800564711548, 0.11326163717233068, 0.1098018450443658, 0.08233351249588383, 0.11859674182661066, 0.09337773297700186, 0.07357095131090925, 0.06540460406145528, 0.05647412926232016, 0.05154557081659896, 0.059184362009014106, 0.026730618188795092]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 310 	 Test accuracy: 87.81 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 311 	 Test accuracy: 88.58 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 312 	 Test accuracy: 88.34 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 313 	 Test accuracy: 87.46000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 314 	 Test accuracy: 88.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 315 	 Test accuracy: 86.83 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 316 	 Test accuracy: 88.07000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 317 	 Test accuracy: 88.46000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 318 	 Test accuracy: 87.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 319 	 Test accuracy: 88.67 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:19<02:38, 79.28s/it] 67%|   | 2/3 [02:54<01:28, 88.60s/it]100%|| 3/3 [03:59<00:00, 77.80s/it]100%|| 3/3 [03:59<00:00, 79.80s/it]


scaleA
 [0.014164488347989444, 0.006135025600946157, 0.006159691716260223, 0.009010059562457396, 0.006146012493311692, 0.0068763259023978465, 0.0045604242546725135, 0.008756853441948624, 0.005961271522656914, 0.00759548641384529, 0.005912035735708928, 0.01071550417661542, 0.007474473046402468, 0.01941836574035909, 0.008642768253027828, 0.019581988398307795, 0.013433059205071063, 0.020350358178188175]
scaleW
 [0.2348745060783571, 0.24301719743686565, 0.3363244977734992, 0.21402369347429118, 0.2977398298672501, 0.16237039048670696, 0.15109837189613085, 0.1184849037964794, 0.10327133766106812, 0.06880111580126036, 0.11997425364321124, 0.08937806314856782, 0.07766211670948912, 0.06255798237705672, 0.06025266419916167, 0.055694680962598136, 0.06553175347267201, 0.02979457694853133]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 320 	 Test accuracy: 87.83 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 321 	 Test accuracy: 88.3 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 322 	 Test accuracy: 88.12 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 323 	 Test accuracy: 87.08 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 324 	 Test accuracy: 87.45 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 325 	 Test accuracy: 88.48 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 326 	 Test accuracy: 88.92999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 327 	 Test accuracy: 88.33 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 328 	 Test accuracy: 89.28 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 329 	 Test accuracy: 89.18 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:24<02:48, 84.45s/it] 67%|   | 2/3 [02:50<01:25, 85.48s/it]100%|| 3/3 [04:00<00:00, 78.20s/it]100%|| 3/3 [04:00<00:00, 80.08s/it]


scaleA
 [0.01611028555431321, 0.007151544346909465, 0.0085523223110927, 0.010634075608870815, 0.006715202064250119, 0.007069652293214264, 0.00632223005933271, 0.009554860947499239, 0.00753238066245679, 0.009549905961096164, 0.0072624858170933955, 0.011720749045953126, 0.007685587015145788, 0.017697275809481133, 0.009407203696859032, 0.018830902738621234, 0.012236159571943782, 0.01923691075514797]
scaleW
 [0.3404858785870019, 0.26846494507635343, 0.39064421097617447, 0.2506300457136453, 0.30970544720784, 0.17693227064160968, 0.16919681786844068, 0.12363569657100693, 0.12705565542478003, 0.08726347856535381, 0.1390242806911086, 0.08422672546512205, 0.07957522808432933, 0.06276464916799436, 0.06337234620582431, 0.05046731110667513, 0.05914994540271464, 0.02586951511986578]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 330 	 Test accuracy: 86.66 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 331 	 Test accuracy: 88.13 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 332 	 Test accuracy: 88.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 333 	 Test accuracy: 88.52 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 334 	 Test accuracy: 89.11 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 335 	 Test accuracy: 88.92999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 336 	 Test accuracy: 89.22 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 337 	 Test accuracy: 88.2 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 338 	 Test accuracy: 88.75 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 339 	 Test accuracy: 89.14 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:43<03:26, 103.08s/it] 67%|   | 2/3 [03:14<01:36, 96.33s/it] 100%|| 3/3 [05:01<00:00, 101.02s/it]100%|| 3/3 [05:01<00:00, 100.44s/it]


scaleA
 [0.016222779187872072, 0.006507159651235405, 0.008104480971352683, 0.011184087749067572, 0.006463580151566964, 0.00820291647093086, 0.005764796657042736, 0.009510309947423108, 0.0060915267218572796, 0.008744861988025198, 0.007137208372958961, 0.011971881048582102, 0.008471390217131197, 0.022059776702582676, 0.009248514317998743, 0.020100895618569512, 0.013499732921236391, 0.018326436697517046]
scaleW
 [0.2680824009894815, 0.25968713161613227, 0.4184583937909178, 0.24542999333849028, 0.32217500233389523, 0.1659523415997132, 0.1598719035611865, 0.1219774118457253, 0.10074307624593994, 0.0760284085411246, 0.1272684448435869, 0.09421200110868334, 0.09081321555983413, 0.07878329518245027, 0.06410908180523685, 0.0566199640807891, 0.06398974587535128, 0.025550250570827878]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 340 	 Test accuracy: 88.87 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 341 	 Test accuracy: 88.1 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 342 	 Test accuracy: 88.66000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 343 	 Test accuracy: 88.96 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 344 	 Test accuracy: 89.44 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 345 	 Test accuracy: 88.86 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 346 	 Test accuracy: 89.25999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 347 	 Test accuracy: 89.49000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 348 	 Test accuracy: 88.75 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 349 	 Test accuracy: 89.18 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:43<03:26, 103.41s/it] 67%|   | 2/3 [03:04<01:30, 90.13s/it] 100%|| 3/3 [04:34<00:00, 90.06s/it]100%|| 3/3 [04:34<00:00, 91.42s/it]


scaleA
 [0.014131586132009695, 0.006418712011307087, 0.006669662528668553, 0.00932810562199642, 0.005961405507137704, 0.006839112568901319, 0.0048781891549258645, 0.00837585614998163, 0.006091910745467212, 0.007457054256136672, 0.00535884806857695, 0.009082249156622696, 0.0071526254205523596, 0.017845172814692657, 0.0072089035729050845, 0.016548390084645384, 0.011380289148161115, 0.017419416562845774]
scaleW
 [0.2309462444749016, 0.20857396557936184, 0.27387874194246137, 0.18586870924255702, 0.2545827858466501, 0.1396439331165439, 0.15182530922709428, 0.11753319246264243, 0.10746437584386648, 0.07459991705090925, 0.1177928618115854, 0.07254361109486111, 0.07711627134604314, 0.06332837780654349, 0.049857127931082795, 0.051372320180210596, 0.05917335257857989, 0.023552321967349393]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 350 	 Test accuracy: 89.35 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 351 	 Test accuracy: 88.75999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 352 	 Test accuracy: 89.27000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 353 	 Test accuracy: 89.57000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 354 	 Test accuracy: 89.31 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 355 	 Test accuracy: 89.12 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 356 	 Test accuracy: 89.32 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 357 	 Test accuracy: 89.14 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 358 	 Test accuracy: 89.55 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 359 	 Test accuracy: 89.25999999999999 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:31<03:02, 91.18s/it] 67%|   | 2/3 [02:42<01:19, 79.32s/it]100%|| 3/3 [04:04<00:00, 80.62s/it]100%|| 3/3 [04:04<00:00, 81.47s/it]


scaleA
 [0.017485726602402333, 0.008298723014002257, 0.009738551870030955, 0.010891937910243033, 0.006900858310906831, 0.008084329593875277, 0.006712175130661679, 0.010751460698292874, 0.008293400612626156, 0.010199963808447253, 0.006107836132791245, 0.010221559659757394, 0.009130170712344508, 0.022712881359069913, 0.010457318414084918, 0.02340986565476877, 0.011931068720708114, 0.021651914761545227]
scaleW
 [0.35183041763569456, 0.29106742987339307, 0.45891965223882725, 0.29674485798915273, 0.3596800594189025, 0.21345767995785506, 0.20026240276636076, 0.15078005225942878, 0.15054175350025986, 0.09983223145774307, 0.1370701691486271, 0.08440862459595046, 0.09885504535551086, 0.08239711305540405, 0.08416258792327858, 0.0671680132054253, 0.06964131629859582, 0.027104468449067973]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 360 	 Test accuracy: 89.12 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 361 	 Test accuracy: 89.32 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 362 	 Test accuracy: 89.35 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 363 	 Test accuracy: 89.39 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 364 	 Test accuracy: 89.52 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 365 	 Test accuracy: 89.42 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 366 	 Test accuracy: 89.5 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 367 	 Test accuracy: 89.27000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 368 	 Test accuracy: 89.56 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 369 	 Test accuracy: 89.52 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:38<03:17, 98.68s/it] 67%|   | 2/3 [02:55<01:25, 85.77s/it]100%|| 3/3 [04:23<00:00, 86.87s/it]100%|| 3/3 [04:23<00:00, 87.88s/it]


scaleA
 [0.01569618328443804, 0.007420782643864536, 0.009549403717044103, 0.012215962032437773, 0.006607137395306824, 0.007734467415517713, 0.006878875829329018, 0.010519736638035937, 0.007101341925237703, 0.009807928541192657, 0.007880967686020892, 0.01276623550630944, 0.009226320084405054, 0.026535671321118642, 0.010103570878548308, 0.023704538465254516, 0.010530453300355685, 0.02090931300515318]
scaleW
 [0.2732078621777092, 0.27515242153623953, 0.42811782312493146, 0.29380125652261224, 0.3249601166342813, 0.21614684521499875, 0.21128358566728597, 0.1596403441009788, 0.12880027153661347, 0.09138427030915318, 0.1495609873408184, 0.11751341147998735, 0.10894224349277144, 0.08825177352611319, 0.07986635080956439, 0.07339266682698964, 0.0666777436011441, 0.029509529372679335]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 370 	 Test accuracy: 89.41 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 371 	 Test accuracy: 89.62 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 372 	 Test accuracy: 89.7 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 373 	 Test accuracy: 89.51 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 374 	 Test accuracy: 89.77000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 375 	 Test accuracy: 89.68 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 376 	 Test accuracy: 89.75999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 377 	 Test accuracy: 89.64999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 378 	 Test accuracy: 89.68 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 379 	 Test accuracy: 89.75 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:36<03:13, 96.81s/it] 67%|   | 2/3 [03:06<01:32, 92.62s/it]100%|| 3/3 [04:29<00:00, 88.08s/it]100%|| 3/3 [04:29<00:00, 89.74s/it]


scaleA
 [0.014260710654674545, 0.00605699260499793, 0.0079449845802715, 0.00933599659204059, 0.006351477326675803, 0.006857307772640983, 0.004943877177623669, 0.009010892671052482, 0.007125781875742816, 0.009726511944850027, 0.005408756055526037, 0.00862123536816702, 0.0076067241385756315, 0.017959582389120533, 0.00765094678249881, 0.017588228520244813, 0.00937922127736546, 0.01799279889667733]
scaleW
 [0.26927868261080934, 0.22832286240142766, 0.3525936331910864, 0.20878157046424495, 0.29724834995689253, 0.16329664429653495, 0.15934674498619852, 0.11828607418920156, 0.13337760229029313, 0.09078817623156162, 0.10940948582102546, 0.07967113864054243, 0.07801560544513901, 0.0643499189286703, 0.057032597796563084, 0.0571663931139744, 0.05282266014206518, 0.02345915196100881]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 380 	 Test accuracy: 89.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 381 	 Test accuracy: 89.53999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 382 	 Test accuracy: 89.60000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 383 	 Test accuracy: 89.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 384 	 Test accuracy: 89.74 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 385 	 Test accuracy: 89.60000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 386 	 Test accuracy: 89.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 387 	 Test accuracy: 89.52 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 388 	 Test accuracy: 89.79 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 389 	 Test accuracy: 89.71000000000001 %
  0%|          | 0/3 [00:00<?, ?it/s] 33%|      | 1/3 [01:18<02:36, 78.36s/it] 67%|   | 2/3 [02:42<01:21, 81.63s/it]100%|| 3/3 [04:02<00:00, 81.12s/it]100%|| 3/3 [04:02<00:00, 80.95s/it]


scaleA
 [0.01360441240293709, 0.005384054563086787, 0.008323347822247022, 0.009368876371310468, 0.004375646573822546, 0.005780526408838057, 0.005630578145046087, 0.008983008915760059, 0.006070268850526743, 0.007358883361690533, 0.006180933860043154, 0.008944919572053338, 0.0075412644666658785, 0.017868047021910232, 0.0059875344192054855, 0.01200264175691841, 0.00968084860981362, 0.015894296448392766]
scaleW
 [0.2774049432980185, 0.25918950598264506, 0.37301389497675713, 0.245516857581383, 0.20071403699430704, 0.14608920520899835, 0.17643595516474161, 0.139255805512326, 0.1048942391821839, 0.06893773139981284, 0.11760924296919632, 0.07444807032189864, 0.07703427638913062, 0.06669092507693246, 0.048039156764427576, 0.04043794659541608, 0.05224332485255332, 0.01942818386383658]

layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 390 	 Test accuracy: 89.69 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 391 	 Test accuracy: 89.71000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 392 	 Test accuracy: 89.75999999999999 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 393 	 Test accuracy: 89.71000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 394 	 Test accuracy: 89.75 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 395 	 Test accuracy: 89.64 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 396 	 Test accuracy: 89.67 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 397 	 Test accuracy: 89.71000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 398 	 Test accuracy: 89.77000000000001 %
layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.0.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.0.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.1.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.1.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv1.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv1.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
Quantizing weight: layer1.2.conv2.weight torch.Size([16, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer1.2.conv2.weight
torch.Size([6, 384]) (384, 6) 192
final weight shape: torch.Size([16, 16, 3, 3])
layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
Quantizing weight: layer2.0.conv1.weight torch.Size([32, 16, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv1.weight
torch.Size([6, 768]) (768, 6) 192
final weight shape: torch.Size([32, 16, 3, 3])
layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.0.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.0.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.1.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.1.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv1.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv1.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
Quantizing weight: layer2.2.conv2.weight torch.Size([32, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer2.2.conv2.weight
torch.Size([6, 1536]) (1536, 6) 192
final weight shape: torch.Size([32, 32, 3, 3])
layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
Quantizing weight: layer3.0.conv1.weight torch.Size([64, 32, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv1.weight
torch.Size([6, 3072]) (3072, 6) 192
final weight shape: torch.Size([64, 32, 3, 3])
layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.0.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.0.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.1.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.1.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv1.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv1.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
Quantizing weight: layer3.2.conv2.weight torch.Size([64, 64, 3, 3])
weighted k-means: False
product quantization with block_size:  6 layer3.2.conv2.weight
torch.Size([6, 6144]) (6144, 6) 192
final weight shape: torch.Size([64, 64, 3, 3])
Bit ratio for compressed layers: 0.9043787508335186
Current epoch: 399 	 Test accuracy: 89.71000000000001 %
The best checkpoint is loaded
Test accuracy: 89.79%
